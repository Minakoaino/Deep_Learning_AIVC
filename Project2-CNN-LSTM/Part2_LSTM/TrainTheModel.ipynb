{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "d1nXtGZcbZIn"
   },
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import os\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy import array\n",
    "from math import ceil\n",
    "from math import log10\n",
    "from numpy import argmax\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Zwt1J6OKchPN"
   },
   "outputs": [],
   "source": [
    "# generate lists of random integers and their sum\n",
    "# n_examples for number of pairs\n",
    "# n_numbers pairs (2)\n",
    "# [ lowest, largest ] \n",
    "def random_sum_pairs(n_examples, n_numbers, lowest, largest):\n",
    "    X, y = list(), list()\n",
    "    for i in range(n_examples):\n",
    "        in_pattern = [randint(lowest, largest) for _ in range(n_numbers)]\n",
    "        out_pattern = sum(in_pattern)\n",
    "        X.append(in_pattern)\n",
    "        y.append(out_pattern)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "NLhdxRaNchRc"
   },
   "outputs": [],
   "source": [
    "# convert data to strings\n",
    "def to_string(X, y, n_numbers, largest):\n",
    "    max_length = n_numbers * ceil(log10(largest + 1)) + n_numbers - 1\n",
    "    Xstr = list()\n",
    "    counter = 0;\n",
    "    for pattern in X:\n",
    "        if randint(lowest, largest) % 2 == 1:\n",
    "            strp = '-'.join([str(n) for n in pattern])\n",
    "            strp = ''.join([' ' for _ in range(max_length - len(strp))]) + strp\n",
    "            Xstr.append(strp)\n",
    "            y[counter] = y[counter] - 2*pattern[1]\n",
    "        else:\n",
    "            strp = '+'.join([str(n) for n in pattern])\n",
    "            strp = ''.join([' ' for _ in range(max_length - len(strp))]) + strp\n",
    "            Xstr.append(strp)\n",
    "        counter = counter+1\n",
    "    max_length = ceil(log10(n_numbers * (largest + 1)))\n",
    "    ystr = list()\n",
    "    for pattern in y:\n",
    "        strp = str(pattern)\n",
    "        strp = ''.join([' ' for _ in range(max_length - len(strp))]) + strp\n",
    "        ystr.append(strp)\n",
    "    return Xstr, ystr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fz4umdN4chTp"
   },
   "outputs": [],
   "source": [
    "# integer encode strings\n",
    "def integer_encode(X, y, alphabet):\n",
    "    char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "    Xenc = list()\n",
    "    for pattern in X:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        Xenc.append(integer_encoded)\n",
    "    yenc = list()\n",
    "    for pattern in y:\n",
    "        integer_encoded = [char_to_int[char] for char in pattern]\n",
    "        yenc.append(integer_encoded)\n",
    "    return Xenc, yenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "NyKBbaruchVh"
   },
   "outputs": [],
   "source": [
    "# one hot encode\n",
    "def one_hot_encode(X, y, max_int):\n",
    "    Xenc = list()\n",
    "    for seq in X:\n",
    "        pattern = list()\n",
    "        for index in seq:\n",
    "            vector = [0 for _ in range(max_int)]\n",
    "            vector[index] = 1\n",
    "            pattern.append(vector)\n",
    "        Xenc.append(pattern)\n",
    "    yenc = list()\n",
    "    for seq in y:\n",
    "        pattern = list()\n",
    "        for index in seq:\n",
    "            vector = [0 for _ in range(max_int)]\n",
    "            vector[index] = 1\n",
    "            pattern.append(vector)\n",
    "        yenc.append(pattern)\n",
    "    return Xenc, yenc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tM2Yjus4chXn"
   },
   "outputs": [],
   "source": [
    "# generate an encoded dataset\n",
    "def generate_data(n_samples, n_numbers, lowest, largest, alphabet):\n",
    "    # generate pairs\n",
    "    X, y = random_sum_pairs(n_samples, n_numbers, lowest,  largest)\n",
    "    # convert to strings\n",
    "    X, y = to_string(X, y, n_numbers, largest)\n",
    "   \n",
    "    # integer encode\n",
    "    X, y = integer_encode(X, y, alphabet)\n",
    "    # one hot encode\n",
    "    X, y = one_hot_encode(X, y, len(alphabet))\n",
    "    # return as numpy arrays\n",
    "    X, y = array(X), array(y)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QduWUtFzeIg0"
   },
   "outputs": [],
   "source": [
    "# invert encoding\n",
    "def invert(seq, alphabet):\n",
    "    int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "    strings = list()\n",
    "    for pattern in seq:\n",
    "        string = int_to_char[argmax(pattern)]\n",
    "        strings.append(string)\n",
    "    return ''.join(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHpMrYGXeIi7",
    "outputId": "46191865-1102-4237-ab9b-7fdd4fef6b36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring the dataset\n",
      "Train set items::5000\n",
      "Operands::2\n",
      "Numbers range: [1,1000\n",
      "Input Length:9\n",
      "Output Length:4\n",
      "Configuring the model\n",
      "Number of batches:10\n",
      "Number of rounds:10\n"
     ]
    }
   ],
   "source": [
    "# define dataset\n",
    "seed(1)\n",
    "print(\"Configuring the dataset\")\n",
    "#Samples to be created for the dataset\n",
    "n_samples = 5000\n",
    "#number of operands\n",
    "n_numbers = 2\n",
    "#lowest number in operation\n",
    "lowest = 1\n",
    "#largest number in operation\n",
    "largest = 1000\n",
    "#text alphabet (decimal digits, +, -)\n",
    "alphabet = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '+', '-', ' ']\n",
    "n_chars = len(alphabet)\n",
    "print(\"Train set items:\", end=\":\")\n",
    "print(n_samples)\n",
    "print(\"Operands:\", end=\":\")\n",
    "print(n_numbers)\n",
    "print(\"Numbers range: [\"+str(lowest)+\",\"+str(largest))\n",
    "#computing text max length\n",
    "n_in_seq_length = n_numbers * ceil(log10(largest + 1)) + n_numbers - 1\n",
    "#computing result max length\n",
    "n_out_seq_length = ceil(log10(n_numbers * (largest + 1)))\n",
    "print(\"Input Length:\"+str(n_in_seq_length))\n",
    "print(\"Output Length:\"+str(n_out_seq_length))\n",
    "print(\"Configuring the model\")\n",
    "# define LSTM configuration\n",
    "n_batch = 10\n",
    "n_epoch = 10\n",
    "nr_epoch = 10\n",
    "print(\"Number of batches:\"+str(n_batch))\n",
    "print(\"Number of rounds:\"+str(n_epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovCujs9ceIlM",
    "outputId": "5c79be48-da29-420e-d96a-c0288314c45b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the model\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 2000)              16112000  \n",
      "                                                                 \n",
      " repeat_vector (RepeatVector  (None, 4, 2000)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 4, 1000)           12004000  \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, 4, 13)            13013     \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 28,129,013\n",
      "Trainable params: 28,129,013\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training the model / 10 rounds of 10 epochs each (each round concerns one different dataset)\n",
      "Epoch: 0:Epoch 1/10\n",
      "500/500 - 186s - loss: 1.9451 - accuracy: 0.2495 - 186s/epoch - 373ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 211s - loss: 1.7083 - accuracy: 0.3349 - 211s/epoch - 421ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 231s - loss: 1.5251 - accuracy: 0.4006 - 231s/epoch - 462ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 241s - loss: 1.4456 - accuracy: 0.4319 - 241s/epoch - 481ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 246s - loss: 1.3983 - accuracy: 0.4518 - 246s/epoch - 492ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 238s - loss: 1.3685 - accuracy: 0.4672 - 238s/epoch - 476ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 245s - loss: 1.3480 - accuracy: 0.4737 - 245s/epoch - 490ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 248s - loss: 1.3324 - accuracy: 0.4835 - 248s/epoch - 496ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 244s - loss: 1.3242 - accuracy: 0.4860 - 244s/epoch - 488ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 244s - loss: 1.3095 - accuracy: 0.4902 - 244s/epoch - 488ms/step\n",
      "Epoch: 1:Epoch 1/10\n",
      "500/500 - 244s - loss: 1.3195 - accuracy: 0.4788 - 244s/epoch - 489ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 241s - loss: 1.2961 - accuracy: 0.4945 - 241s/epoch - 482ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 248s - loss: 1.2907 - accuracy: 0.4981 - 248s/epoch - 496ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 248s - loss: 1.2851 - accuracy: 0.5057 - 248s/epoch - 496ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 241s - loss: 1.2710 - accuracy: 0.5078 - 241s/epoch - 482ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 254s - loss: 1.2623 - accuracy: 0.5125 - 254s/epoch - 507ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 267s - loss: 1.2455 - accuracy: 0.5189 - 267s/epoch - 535ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 253s - loss: 1.2433 - accuracy: 0.5259 - 253s/epoch - 506ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 265s - loss: 1.2373 - accuracy: 0.5278 - 265s/epoch - 531ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 251s - loss: 1.2296 - accuracy: 0.5308 - 251s/epoch - 502ms/step\n",
      "Epoch: 2:Epoch 1/10\n",
      "500/500 - 240s - loss: 1.2748 - accuracy: 0.5080 - 240s/epoch - 480ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 249s - loss: 1.2519 - accuracy: 0.5177 - 249s/epoch - 499ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 241s - loss: 1.2403 - accuracy: 0.5210 - 241s/epoch - 482ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 244s - loss: 1.2166 - accuracy: 0.5357 - 244s/epoch - 489ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 309s - loss: 1.2077 - accuracy: 0.5426 - 309s/epoch - 619ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 272s - loss: 1.1861 - accuracy: 0.5553 - 272s/epoch - 543ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 209s - loss: 1.1551 - accuracy: 0.5665 - 209s/epoch - 417ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 195s - loss: 1.1098 - accuracy: 0.5814 - 195s/epoch - 390ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 201s - loss: 1.0413 - accuracy: 0.6085 - 201s/epoch - 401ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 201s - loss: 1.0108 - accuracy: 0.6195 - 201s/epoch - 402ms/step\n",
      "Epoch: 3:Epoch 1/10\n",
      "500/500 - 208s - loss: 1.0913 - accuracy: 0.5750 - 208s/epoch - 415ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 202s - loss: 1.0100 - accuracy: 0.6107 - 202s/epoch - 405ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 198s - loss: 0.9547 - accuracy: 0.6378 - 198s/epoch - 395ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 198s - loss: 0.8948 - accuracy: 0.6586 - 198s/epoch - 396ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 203s - loss: 0.8582 - accuracy: 0.6746 - 203s/epoch - 407ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 198s - loss: 0.7876 - accuracy: 0.7040 - 198s/epoch - 396ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 197s - loss: 0.7358 - accuracy: 0.7265 - 197s/epoch - 393ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 203s - loss: 0.6881 - accuracy: 0.7450 - 203s/epoch - 406ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 196s - loss: 0.6383 - accuracy: 0.7681 - 196s/epoch - 392ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 198s - loss: 0.5943 - accuracy: 0.7870 - 198s/epoch - 396ms/step\n",
      "Epoch: 4:Epoch 1/10\n",
      "500/500 - 189s - loss: 0.8868 - accuracy: 0.6585 - 189s/epoch - 377ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 189s - loss: 0.7604 - accuracy: 0.7102 - 189s/epoch - 378ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 210s - loss: 0.6990 - accuracy: 0.7398 - 210s/epoch - 421ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 198s - loss: 0.6605 - accuracy: 0.7589 - 198s/epoch - 396ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 196s - loss: 0.6109 - accuracy: 0.7786 - 196s/epoch - 392ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 196s - loss: 0.5362 - accuracy: 0.8127 - 196s/epoch - 392ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 210s - loss: 0.5047 - accuracy: 0.8198 - 210s/epoch - 419ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 204s - loss: 0.4385 - accuracy: 0.8504 - 204s/epoch - 408ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 198s - loss: 0.3990 - accuracy: 0.8639 - 198s/epoch - 395ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 204s - loss: 0.3378 - accuracy: 0.8881 - 204s/epoch - 409ms/step\n",
      "Epoch: 5:Epoch 1/10\n",
      "500/500 - 200s - loss: 0.7704 - accuracy: 0.7043 - 200s/epoch - 399ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 211s - loss: 0.5506 - accuracy: 0.7950 - 211s/epoch - 422ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 206s - loss: 0.4639 - accuracy: 0.8353 - 206s/epoch - 412ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 200s - loss: 0.3829 - accuracy: 0.8694 - 200s/epoch - 401ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 196s - loss: 0.3163 - accuracy: 0.8961 - 196s/epoch - 391ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 209s - loss: 0.2568 - accuracy: 0.9187 - 209s/epoch - 418ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 211s - loss: 0.2051 - accuracy: 0.9383 - 211s/epoch - 421ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 204s - loss: 0.1789 - accuracy: 0.9510 - 204s/epoch - 408ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 201s - loss: 0.1484 - accuracy: 0.9591 - 201s/epoch - 403ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 198s - loss: 0.0992 - accuracy: 0.9764 - 198s/epoch - 396ms/step\n",
      "Epoch: 6:Epoch 1/10\n",
      "500/500 - 196s - loss: 0.5192 - accuracy: 0.7988 - 196s/epoch - 393ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 197s - loss: 0.2456 - accuracy: 0.9172 - 197s/epoch - 394ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 196s - loss: 0.1576 - accuracy: 0.9563 - 196s/epoch - 392ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 196s - loss: 0.1250 - accuracy: 0.9677 - 196s/epoch - 392ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 194s - loss: 0.0908 - accuracy: 0.9801 - 194s/epoch - 388ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 199s - loss: 0.0990 - accuracy: 0.9736 - 199s/epoch - 397ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 200s - loss: 0.1001 - accuracy: 0.9730 - 200s/epoch - 400ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 194s - loss: 0.0995 - accuracy: 0.9719 - 194s/epoch - 388ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 191s - loss: 0.0426 - accuracy: 0.9919 - 191s/epoch - 383ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 191s - loss: 0.0321 - accuracy: 0.9936 - 191s/epoch - 383ms/step\n",
      "Epoch: 7:Epoch 1/10\n",
      "500/500 - 190s - loss: 0.4193 - accuracy: 0.8501 - 190s/epoch - 379ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 186s - loss: 0.1412 - accuracy: 0.9552 - 186s/epoch - 371ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 182s - loss: 0.0728 - accuracy: 0.9826 - 182s/epoch - 364ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 184s - loss: 0.0425 - accuracy: 0.9926 - 184s/epoch - 368ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 183s - loss: 0.0454 - accuracy: 0.9903 - 183s/epoch - 367ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 182s - loss: 0.1168 - accuracy: 0.9637 - 182s/epoch - 363ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 181s - loss: 0.0639 - accuracy: 0.9817 - 181s/epoch - 362ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 183s - loss: 0.0354 - accuracy: 0.9919 - 183s/epoch - 365ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 186s - loss: 0.0192 - accuracy: 0.9966 - 186s/epoch - 372ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 182s - loss: 0.0848 - accuracy: 0.9747 - 182s/epoch - 365ms/step\n",
      "Epoch: 8:Epoch 1/10\n",
      "500/500 - 183s - loss: 0.3023 - accuracy: 0.8957 - 183s/epoch - 366ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "500/500 - 181s - loss: 0.0888 - accuracy: 0.9737 - 181s/epoch - 361ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 182s - loss: 0.0430 - accuracy: 0.9906 - 182s/epoch - 363ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 183s - loss: 0.0258 - accuracy: 0.9955 - 183s/epoch - 365ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 183s - loss: 0.0734 - accuracy: 0.9790 - 183s/epoch - 366ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 184s - loss: 0.0633 - accuracy: 0.9802 - 184s/epoch - 368ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 186s - loss: 0.0513 - accuracy: 0.9843 - 186s/epoch - 373ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 213s - loss: 0.0602 - accuracy: 0.9823 - 213s/epoch - 426ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 199s - loss: 0.0383 - accuracy: 0.9888 - 199s/epoch - 397ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 190s - loss: 0.0158 - accuracy: 0.9969 - 190s/epoch - 379ms/step\n",
      "Epoch: 9:Epoch 1/10\n",
      "500/500 - 197s - loss: 0.2516 - accuracy: 0.9138 - 197s/epoch - 395ms/step\n",
      "Epoch 2/10\n",
      "500/500 - 206s - loss: 0.0817 - accuracy: 0.9734 - 206s/epoch - 413ms/step\n",
      "Epoch 3/10\n",
      "500/500 - 195s - loss: 0.0295 - accuracy: 0.9929 - 195s/epoch - 390ms/step\n",
      "Epoch 4/10\n",
      "500/500 - 196s - loss: 0.0121 - accuracy: 0.9984 - 196s/epoch - 393ms/step\n",
      "Epoch 5/10\n",
      "500/500 - 203s - loss: 0.0086 - accuracy: 0.9988 - 203s/epoch - 405ms/step\n",
      "Epoch 6/10\n",
      "500/500 - 204s - loss: 0.0762 - accuracy: 0.9777 - 204s/epoch - 409ms/step\n",
      "Epoch 7/10\n",
      "500/500 - 213s - loss: 0.0748 - accuracy: 0.9765 - 213s/epoch - 426ms/step\n",
      "Epoch 8/10\n",
      "500/500 - 203s - loss: 0.0723 - accuracy: 0.9800 - 203s/epoch - 407ms/step\n",
      "Epoch 9/10\n",
      "500/500 - 201s - loss: 0.0163 - accuracy: 0.9970 - 201s/epoch - 401ms/step\n",
      "Epoch 10/10\n",
      "500/500 - 214s - loss: 0.0092 - accuracy: 0.9983 - 214s/epoch - 427ms/step\n"
     ]
    }
   ],
   "source": [
    "# create LSTM\n",
    "print(\"Creating the model\")\n",
    "model = Sequential()\n",
    "#Adding the input LSTM layer\n",
    "model.add(LSTM(2*largest, input_shape=(n_in_seq_length, n_chars)))\n",
    "model.add(RepeatVector(n_out_seq_length))\n",
    "model.add(LSTM(largest, return_sequences=True))\n",
    "#Adding Dense layer for output\n",
    "model.add(TimeDistributed(Dense(n_chars, activation='softmax')))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "print(\"Training the model / \"+str(n_epoch)+\" rounds of \"+str(nr_epoch)+\" epochs each (each round concerns one different dataset)\")\n",
    "#training the model in n_epoch round of nr_epoch epochs each\n",
    "for i in range(n_epoch):\n",
    "    X, y = generate_data(n_samples, n_numbers, lowest, largest, alphabet)\n",
    "    print(\"Epoch: \"+str(i),end=\":\")\n",
    "    history = model.fit(X, y, epochs=nr_epoch, batch_size=n_batch, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_4iv96xAchZ8"
   },
   "outputs": [],
   "source": [
    "#Create outputData folder\n",
    "outdir = './OutputData'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "#store model in outputdata folder\n",
    "model.save(\"OutputData/ModelLSTM.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on some new patterns\n",
    "X, y = generate_data(n_samples, n_numbers, lowest, largest, alphabet)\n",
    "\n",
    "result = model.predict(X, batch_size=n_batch, verbose=0)\n",
    "# calculate error\n",
    "expected = [invert(x, alphabet) for x in y]\n",
    "predicted = [invert(x, alphabet) for x in result]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
